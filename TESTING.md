# ğŸ§ª GuÃ­a de Testing - AnaliticaIA

DocumentaciÃ³n completa para ejecutar tests automatizados del microservicio.

---

## ğŸ“‹ Tests Implementados

### **Suite Completa: 15 Tests**

#### **Tests BÃ¡sicos (3)**
- âœ… Health Check
- âœ… Root Endpoint  
- âœ… API Documentation (Swagger)

#### **Tests de PredicciÃ³n de Demanda (4)**
- âœ… PredicciÃ³n producto especÃ­fico (semana)
- âœ… PredicciÃ³n producto especÃ­fico (mes)
- âœ… PredicciÃ³n todos los productos
- âŒ Producto inexistente (debe fallar)

#### **Tests de SegmentaciÃ³n (4)**
- âœ… SegmentaciÃ³n 3 clusters
- âœ… SegmentaciÃ³n 4 clusters
- âœ… Segmento cliente especÃ­fico
- âŒ Cliente inexistente (debe fallar)

#### **Tests de OptimizaciÃ³n de Rutas (4)**
- âœ… OptimizaciÃ³n 1 vehÃ­culo
- âœ… OptimizaciÃ³n 2 vehÃ­culos
- âœ… Rutas histÃ³ricas
- âŒ Pedido inexistente (debe fallar)

---

## ğŸš€ CÃ³mo Ejecutar los Tests

### **OpciÃ³n 1: Todo-en-Uno (Recomendada)** â­

Entrena los modelos Y ejecuta los tests automÃ¡ticamente:

```bash
# Dentro del contenedor Docker
docker-compose exec analiticaia-api python scripts/test_all.py

# O ejecutar como comando Ãºnico
docker-compose run --rm analiticaia-api python scripts/test_all.py
```

**QuÃ© hace:**
1. ğŸ“ Entrena los 3 modelos ML
2. â³ Espera 5 segundos
3. ğŸ§ª Ejecuta los 15 tests
4. ğŸ“Š Muestra resumen

**DuraciÃ³n:** ~2-3 minutos

---

### **OpciÃ³n 2: Solo Tests**

Si ya entrenaste los modelos:

```bash
# Dentro del contenedor
docker-compose exec analiticaia-api python scripts/test_api.py

# O desde fuera (si API estÃ¡ en localhost:8000)
python scripts/test_api.py
```

---

### **OpciÃ³n 3: Test Manual con Curl**

#### Health Check
```bash
curl http://localhost:8000/health
```

#### PredicciÃ³n de Demanda
```bash
curl -X POST http://localhost:8000/api/v1/predict/demand \
  -H "Content-Type: application/json" \
  -d '{"producto_id": 1, "periodo": "semana"}'
```

#### SegmentaciÃ³n
```bash
curl -X POST http://localhost:8000/api/v1/segment/customers \
  -H "Content-Type: application/json" \
  -d '{"num_clusters": 3}'
```

#### OptimizaciÃ³n de Rutas
```bash
curl -X POST http://localhost:8000/api/v1/optimize/routes \
  -H "Content-Type: application/json" \
  -d '{"pedidos": [1, 2, 3], "vehiculos": 1}'
```

---

## ğŸ“Š Salida Esperada

### **Test Exitoso**
```
ğŸ§ª Test: PredicciÃ³n de Demanda - Producto 1
======================================================================
ğŸ“¡ POST http://localhost:8000/api/v1/predict/demand
ğŸ“¤ Request: {
  "producto_id": 1,
  "periodo": "semana"
}
ğŸ“¥ Status: 200
âœ… Status correcto: 200
ğŸ“¦ Response:
{
  "producto_id": 1,
  "periodo": "semana",
  "cantidad_estimada": 150.5,
  "intervalo_confianza": {
    "lower": 120.4,
    "upper": 180.6
  },
  "confianza": 85.5
}
âœ… TEST PASADO: PredicciÃ³n de Demanda - Producto 1
```

### **Resumen Final**
```
======================================================================
ğŸ“Š RESUMEN DE TESTS
======================================================================
âœ… Pasados: 15
âŒ Fallados: 0
ğŸ“ˆ Total: 15
ğŸ¯ Tasa de Ã©xito: 100.00%
======================================================================

ğŸ‰ Â¡TODOS LOS TESTS PASARON!
```

---

## ğŸ› Troubleshooting

### Error: "Connection refused"
**Causa:** La API no estÃ¡ corriendo  
**SoluciÃ³n:**
```bash
docker-compose ps  # Verificar estado
docker-compose up  # Iniciar si estÃ¡ detenida
```

### Error: "Modelo no entrenado"
**Causa:** No se han entrenado los modelos ML  
**SoluciÃ³n:**
```bash
docker-compose run --rm analiticaia-api python scripts/train_models.py
```

### Error: "Producto/Cliente/Pedido no encontrado"
**Causa:** Los IDs en los tests no existen en tu BD  
**SoluciÃ³n:**
1. Verificar datos: `docker-compose logs analiticaia-sync`
2. Modificar IDs en `scripts/test_api.py` segÃºn tus datos
3. O agregar datos de prueba manualmente

### Error: "Bin labels must be one fewer than..."
**Causa:** Bug en segmentaciÃ³n (YA ARREGLADO)  
**SoluciÃ³n:** Pull el cÃ³digo actualizado

### Tests lentos
**Causa:** Modelos grandes o DB lenta  
**OptimizaciÃ³n:**
- Usa menos datos de entrenamiento
- Reduce `n_clusters` en segmentaciÃ³n
- Verifica conexiÃ³n a Supabase

---

## ğŸ“ Modificar Tests

### Agregar un Nuevo Test

Edita `scripts/test_api.py`:

```python
# Test 16: Mi nuevo test
tester.test(
    name="Mi Nuevo Test",
    method="POST",  # GET o POST
    url=f"{API_V1}/mi-endpoint",
    data={"param": "valor"},  # Solo para POST
    expected_status=200
)
```

### Cambiar IDs de Prueba

```python
# Usar IDs que existan en tu BD
data={
    "producto_id": 1,    # Cambiar por ID existente
    "periodo": "semana"
}
```

### Agregar Validaciones Personalizadas

```python
def test_custom(self, name, url, validator_func):
    response = requests.get(url)
    if validator_func(response.json()):
        self.passed += 1
    else:
        self.failed += 1
```

---

## ğŸ”„ CI/CD Integration

### GitHub Actions

```yaml
name: API Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Build and Start Services
        run: docker-compose up -d
      
      - name: Wait for API
        run: sleep 30
      
      - name: Run Tests
        run: docker-compose exec -T analiticaia-api python scripts/test_all.py
      
      - name: Cleanup
        run: docker-compose down
```

---

## ğŸ“ˆ MÃ©tricas de Cobertura

| MÃ³dulo | Endpoints | Tests | Cobertura |
|--------|-----------|-------|-----------|
| Health | 1 | 1 | 100% |
| Demanda | 2 | 4 | 100% |
| SegmentaciÃ³n | 2 | 4 | 100% |
| Rutas | 2 | 4 | 100% |
| **TOTAL** | **7** | **15** | **100%** |

---

## ğŸ¯ Mejores PrÃ¡cticas

### Antes de Cada Test
1. âœ… Entrenar modelos
2. âœ… Verificar que la API estÃ¡ corriendo
3. âœ… Confirmar que hay datos en Supabase
4. âœ… Revisar logs si algo falla

### DespuÃ©s de Cambios
1. âœ… Ejecutar `test_all.py`
2. âœ… Verificar que todos los tests pasan
3. âœ… Revisar logs de errores
4. âœ… Actualizar tests si cambiaste endpoints

### Tests en ProducciÃ³n
âš ï¸ **NO ejecutes tests destructivos en producciÃ³n**
- Usa datos de prueba
- Ambiente separado
- Endpoints de solo lectura

---

## ğŸ› ï¸ Herramientas Adicionales

### Postman Collection

Importa esta colecciÃ³n para tests manuales:

```json
{
  "info": {
    "name": "AnaliticaIA Tests",
    "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
  },
  "item": [
    {
      "name": "Health Check",
      "request": {
        "method": "GET",
        "url": "http://localhost:8000/health"
      }
    },
    {
      "name": "Predict Demand",
      "request": {
        "method": "POST",
        "url": "http://localhost:8000/api/v1/predict/demand",
        "body": {
          "mode": "raw",
          "raw": "{\"producto_id\": 1, \"periodo\": \"semana\"}"
        }
      }
    }
  ]
}
```

### Swagger UI (Recomendado)

Mejor herramienta para tests manuales:
```
http://localhost:8000/docs
```

**Ventajas:**
- âœ… Interfaz visual
- âœ… Tests interactivos
- âœ… DocumentaciÃ³n integrada
- âœ… ValidaciÃ³n automÃ¡tica

---

## ğŸ“š Logs de Tests

Los logs se guardan en:
- `logs/test_api.log` - Tests individuales
- `logs/test_all.log` - Pipeline completo

Ver logs en tiempo real:
```bash
tail -f logs/test_api.log
```

---

## âœ… Checklist Pre-Deploy

Antes de deployar a producciÃ³n:

- [ ] âœ… Todos los tests pasan
- [ ] âœ… Modelos entrenados con datos reales
- [ ] âœ… SincronizaciÃ³n funcionando
- [ ] âœ… Health check retorna 200
- [ ] âœ… Swagger UI accesible
- [ ] âœ… Logs sin errores crÃ­ticos
- [ ] âœ… Base de datos con datos de producciÃ³n
- [ ] âœ… Variables de entorno configuradas

---

**Â¿Preguntas?** Revisa los logs o ejecuta con `--verbose` para mÃ¡s detalles ğŸš€
